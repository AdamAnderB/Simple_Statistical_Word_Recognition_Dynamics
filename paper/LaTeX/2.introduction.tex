\noindent\section{Introduction}

Spoken word recognition unfolds incrementally, as soon as acoustic information becomes available, listeners activate multiple lexical candidates and continuously update these hypotheses as the signal unfolds \cite{Allopenna1998,Tanenhaus1995,McMurrayAslin2005,ToscanoMcMurray2010,KuperbergJaeger2016}. Classic computational models capture these time-varying dynamics by linking acoustic evidence to lexical activation. TRACE models spoken word recognition as interactive activation among features, phonemes, and words, with competition arising from continuous input over time \cite{McClellandElman1986}. Shortlist and Shortlist B extend this architecture using probabilistic inference to map uncertain acoustic cues to lexical hypotheses \cite{Norris1994,NorrisMcQueen2008}. \texttt{jTRACE} provides an explicit, time-sliced implementation of TRACE that has been widely used for modeling fine-grained activation dynamics \cite{Strauss2007}.

However, the acoustic signal contains far more than segmental features. Prosodic cues---including fundamental frequency (F0), duration, intensity, and spectral tilt---also evolve continuously and strongly shape lexical activation. For example, Mandarin uses four lexical tones (high, rising, low-dipping, falling), which are carried primarily by dynamic F0 trajectories \cite{Yip2002, Xu1997}. A large literature using eye-tracking shows that tone information constrains lexical activation early, even before segmental uniqueness \cite{MalinsJoanisse2010,ShenFroud2019}. 

Italian provides a parallel case for stress. Italian trisyllabic words typically bear antepenultimate (first syllable stress) or penultimate (second syllable stress) realized through systematic differences in duration, F0, amplitude, and spectral tilt across syllables \cite{CutlerClifton1984}. Experimental work shows that listeners actively exploit these acoustic cues during recognition, and that prosodic misalignment produces measurable processing costs \cite{SulpizioMcQueen2012,Sulpizio2013}. Acoustic studies further reveal robust differences between individual cue sensitivity (e.g., duration and F0) and stress processing, demonstrating that stress cues are multidimensional and gradient rather than categorical \cite{BramlettWiener2025}.

Despite these empirical advances, computational models have struggled to incorporate prosodic cues in a principled, incremental manner. TRACE and Shortlist---though foundational---depend on fixed time slices aligned with segmental structure, making it difficult to represent cues like tone or stress whose diagnostic value emerges over longer temporal windows \cite{McClellandElman1986,Norris1994,NorrisMcQueen2008,Francis_2008}. \texttt{jTRACE}-based attempts to simulate tone typically require encoding tonal categories as discrete, segment-aligned features, despite the fact that F0 trajectories unfold continuously and interact with co-articulation and tone sandhi \cite{Strauss2007,ShuaiMalins2017, bramlett22_speechprosody}. As a result, current models either (a) discretize inherently continuous prosodic cues or (b) ignore their incremental availability, limiting their ability to capture empirical time-course data.

A parallel modeling limitation is in how existing models integrate multiple acoustic dimensions. Frameworks such as TRACE and Shortlist assume representational formats in which segmental and prosodic information must be aligned to predefined symbolic units. This architecture makes it difficult to model recognition in languages where cues unfold asynchronously or contribute evidence on different timescales. Tone contours develop over the entire syllable, stress cues emerge from duration and spectral relationships across multiple syllables, and segmental cues often arrive earlier but are not always decisive. Even time-sliced implementations such as \texttt{jTRACE} inherit this segment-aligned structure, requiring prosodic cues to be discretized into feature-like units \cite{Strauss2007}. As a consequence, no widely used model provides a transparent mechanism for combining heterogeneous acoustic dimensions---segmental, tonal, and stress-based---into a single incremental activation trajectory.

To address these challenges, a small, modular computational framework was built to provide a transparent and tractable computational tool for examining how continuous segmental and prosodic cues jointly shape incremental spoken-word recognition. Using a simple architecture with adjustable listener-specific cue weightings, the model is designed to approximate well-established empirical patterns across three languages: classic cohort dynamics in English \cite{Allopenna1998}, tone-driven divergence in Mandarin (e.g., \cite{MalinsJoanisse2010,ShenFroud2019}), and stress-based lexical competition in Italian \cite{SulpizioMcQueen2012,BramlettWiener2025}. By exploring whether these cross-linguistic signatures can be captured within a single continuous-time framework, we aim to highlight what can be achieved with minimal assumptions and without discretizing prosodic cues. The simulations that follow therefore serve as a proof of concept, illustrating how continuous cue integration may offer a unified perspective on real-time lexical activation in typologically diverse languages.

In English, we aim to reproduce classic cohort and rhyme effects in which competitors sharing initial segments show early or mid-word coactivation, respectively, that diverges as segmental information accumulates (e.g., beetle, beaker, speaker, carriage) \cite{Allopenna1998}. 

In Mandarin, we target toneâ€“similarity patterns in which tones with comparable F0 shapes show sustained coactivation. For example, rising (T2) and dipping (T3) tones share similar early pitch movement and therefore exhibit extended initial overlap, whereas high-level (T1) and falling (T4) tones diverge earlier \cite{Gandour2000}. More generally, tones with globally rising contours (T2, T3) tend to maintain competition longer than tones with high or falling contours (T1, T4) \cite{MalinsJoanisse2010,ShenFroud2019}. 

In Italian, we model stress-based competition in which duration and F0 patterns across syllables activate antepenultimate and penultimate stress competitors, but not segment competitors \cite{SulpizioMcQueen2012,BramlettWiener2025}. By specifying these empirically grounded benchmarks, we establish clear expectations for the activation dynamics in each simulation, allowing the Results section to directly evaluate whether simple continuous cue integration can reproduce these well-known time-course signatures.

