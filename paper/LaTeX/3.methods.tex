\section{Methods}
Each simulation uses a minimal lexicon containing two contrastive word types. Words are represented as multidimensional Gaussian distributions over relevant acoustic dimensions or approximations. For each dimension, lexical items are defined by a mean, a variance, and a time-course function specifying how that cue evolves across successive time steps. Different simulations draw on distinct acoustic contrasts appropriate for the language under study.

For English, the simulation uses a generic segmental evidence trajectory rather than a single phonetic cue such as VOT. Each word is represented by a continuous acoustic signature that unfolds across its onset, vowel, and coda portions, with diagnostic information increasing as the segmental structure becomes clearer over time. This captures the fact that words like Beaker, Beetle, Speaker, and Carriage differ at multiple points in the acoustic stream, and cannot be placed on a single phonetic dimension. Figure~\ref{fig:english_desc} illustrates these continuous segmental trajectories for the four English items, showing how the onset, vowel, and coda each contribute incrementally to lexical differentiation. Example is modified from \cite{Allopenna1998}.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.35]{visuals/english.png}
  \caption{Segmental trajectories for English items.}
  \label{fig:english_desc}
\end{figure}

The Mandarin simulation represents tone and segment. Whereas segments are held constant (i.e., ma), each lexical tone is a continuous F0 trajectory that unfolds across the duration of the syllable. Rather than treating tone as a discrete category or as a static F0 level, the model encodes the shape of the pitch contour—high-level, rising, dipping, or falling—as a time‐dependent distribution. At each time step, the listener receives a slice of F0 information that gradually sharpens the posterior over tone competitors. This structure captures the fact that tonal contrasts become informative early but not instantly, with diagnostic differences accumulating as the contour develops. Figure~\ref{fig:mandarin_desc} illustrates these continuous trajectories for the four Mandarin tones, highlighting their separation in F0 while showing that secondary acoustic cues contribute minimally in comparison. Tonal acoustics are adapted from \cite{Xu1997}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.35]{visuals/mandarin.png}
\caption{Acoustic cue trajectories for Mandarin tones, showing clear separation in F0 contours and minimal contrast in secondary cues.}
\label{fig:mandarin_desc}
\end{figure}

The Italian stress simulation represents both unique segmental information (like the English model) and prosodic patterns (like the Mandarin Model). Crucially, both the stress pattern and the segmental composition differ within and between words, such that competitors diverge in how duration and F0 unfold across the trisyllabic sequence. Each word is encoded as a set of continuous trajectories over these dimensions, with stressed syllables exhibiting characteristic lengthening and pitch movement relative to their unstressed neighbors. Because duration ratios and F0 cues emerge across multiple syllables rather than at a single moment, the model treats these prosodic patterns as time-dependent distributions that accumulate diagnostic evidence gradually. This structure captures the fact that stress contrasts become informative only once the relevant syllable is encountered, producing a delayed but decisive shift in lexical activation for listeners who weight duration or F0 strongly. Figure~\ref{fig:italian_desc} illustrates these continuous trajectories for the Italian items, showing how duration, F0, and segmental cues jointly shape competition between stress-based lexical alternatives. Stress and segment acoustics are adapted from \cite{BramlettWiener2025}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{visuals/italian.png}
\caption{Cue trajectories for Italian items across duration, F0, and segmental dimensions, highlighting strong segmental differentiation and the emergence of stress-based prosodic cues over time.}
\label{fig:italian_desc}
\end{figure}

In terms of the modeling architecture, the framework consists of three core components—a language user that supplies cue weights and representational precision (priors), a lexicon that encodes words as continuous cue trajectories, and an incremental recognizer that updates activation as new acoustic evidence arrives (see Figure~\ref{fig:schematic}). This general structure parallels classic incremental competition models such as TRACE \cite{McClellandElman1986} and Shortlist/Shortlist~B \cite{Norris1994,NorrisMcQueen2008}, but differs from them in treating all acoustic cues—segmental and prosodic—as continuous, time-varying evidence rather than discretized features.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{visuals/schematic.png}
  \caption{Schematic of the incremental word-recognition model. The lexicon encodes words as trajectories over continuous cues, and the language user provides cue weights, noise, and priors. The incremental recognizer combines these to extract target cues, compare them to lexical expectations, and update posteriors over words. The inset shows the incremental update cycle executed at each time step: cue input, comparison to lexical expectations, accumulation of log-likelihoods, and a softmax transformation to posterior activation.}
  \label{fig:schematic}
\end{figure}

At each time step $t$, the model receives a slice of acoustic evidence along one or more dimensions, such as VOT, F0, or duration. For each candidate word $w$ and each dimension $d$, the likelihood of observing the current acoustic value is computed as:
\[
\ell_{w,d}(t) = P(x_d(t) \mid \mu_{w,d}, \sigma_{w,d}).
\]

This structure accommodates cues that unfold over time, such as Mandarin tone contours or stress-based duration patterns, and therefore allows the model to integrate prosodic information continuously rather than discretizing cues into segment-aligned categories.

Differences between listener groups are simulated by introducing cue-attention weights that scale the contribution of each dimension. The likelihood for each word is combined across dimensions using the weighting function:
\[
\ell_{w}(t) = \prod_d \ell_{w,d}(t)^{\alpha_d}.
\]

The parameter $\alpha_d$ increases or decreases reliance on that cue and can be set near zero for listeners who lack a category system for a given dimension. Representational precision is manipulated by adjusting the variances of the Gaussian categories, allowing the model to simulate listeners with sharp, native-like categories or more diffuse, noisy categories. 

Lexical activation is updated incrementally using a softmax competition mechanism:
\[
A_{w}(t) = \frac{\exp(\beta \cdot \log \ell_{w}(t))}{\sum_{w'} \exp(\beta \cdot \log \ell_{w'}(t))}.
\]

The parameter $\beta$ determines the sharpness of competition across lexical candidates. This update process yields smooth activation trajectories that directly reflect the evolving acoustic evidence over time, closely mirroring the structure of time-course data such as eye-movement patterns or gating results.The current implementation is strictly feedforward: activation is updated incrementally from incoming acoustic evidence without feedback from lexical to acoustic representations, allowing us to isolate the contribution of continuous cue integration.

Finally, each of the three simulations instantiates this same architecture using cue trajectories appropriate to the target contrast. The English simulation uses time-varying segmental evidence, the Mandarin simulation uses continuous F0 contours for tone, and the Italian simulation uses duration and F0 patterns distributed across syllables. All three rely on the same likelihood computations, the same cue-weighting mechanism, and the same incremental softmax update. This ensures that any differences in activation dynamics arise from properties of the cues and listener parameters rather than from differences in model structure. Simulation code was implemented in \texttt{R} using custom functions to generate cue trajectories, compute likelihoods, and update posteriors at each time step. All simulation code was implemented in \texttt{R} and is stored on OSF: {\url{https://osf.io/7ab2u/overview?view_only=9c33713e1a2a4c1eb81253ae4e5a822b}.
